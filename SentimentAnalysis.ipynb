{
 "cells": [
  {
   "cell_type": "code",
   "id": "9600cd7acd291a22",
   "metadata": {},
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(token=hf_token)\n",
    "\n",
    "# keywords for “guidance” events and “layoff” events\n",
    "GUIDANCE_KEYWORDS = [\n",
    "    r'\\bguidance\\b',\n",
    "    r'\\bforecast\\b',\n",
    "    r'\\brevis(ed|ion)?\\b',\n",
    "    r'\\bupward\\b',\n",
    "    r'\\bdownward\\b',\n",
    "    r'\\braise[sd]?\\b',\n",
    "    r'\\blower(ed)?\\b',\n",
    "    r'\\bexpect(ed|ation)?\\b',\n",
    "]\n",
    "\n",
    "LAYOFF_KEYWORDS = [\n",
    "    r'\\blayoff(ed|s)?\\b',\n",
    "    r'\\bjob cut(s)?\\b',\n",
    "    r'\\breduc(e|ing) (workforce|staff)\\b',\n",
    "    r'\\brestructur(ing|e)?\\b',\n",
    "    r'\\bdownsiz(e|ing)\\b',\n",
    "]\n",
    "\n",
    "SEEDS = {\n",
    "    \"guidance_up\": [\n",
    "        \"We are raising our full-year guidance.\",\n",
    "        \"Outlook for the quarter has been increased.\"\n",
    "    ],\n",
    "    \"guidance_down\": [\n",
    "        \"We are lowering our guidance.\",\n",
    "        \"Full-year outlook was reduced.\"\n",
    "    ],\n",
    "    \"layoff\": [\n",
    "        \"We will reduce headcount across several departments.\",\n",
    "        \"The company has initiated workforce reductions.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Folder of your 8-K .txt files:\n",
    "INPUT_FOLDER = \"wrds_clean_filings_1994\"\n",
    "\n",
    "# Cosine‐similarity threshold for flagging\n",
    "THRESHOLD = 0.6\n",
    "\n",
    "# Minimum paragraph length to consider\n",
    "MIN_LEN = 50\n",
    "\n",
    "mapping = {\n",
    "    \"negative\": -10,\n",
    "    \"neutral\": 0,\n",
    "    \"positive\": 10\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "# 2. Load model and precompute seed embeddings ------------------------\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "# Build SBERT pipeline around FinBERT with mean pooling\n",
    "word_embedding_model = models.Transformer(\"ProsusAI/finbert\")\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "print(\"Encoding seed sentences…\")\n",
    "seed_embeddings = {\n",
    "    label: model.encode(texts, convert_to_tensor=True)\n",
    "    for label, texts in SEEDS.items()\n",
    "}"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def is_guidance_para(para):\n",
    "    text = para.lower()\n",
    "    for kw in GUIDANCE_KEYWORDS:\n",
    "        if re.search(kw, text):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_layoff_para(para):\n",
    "    text = para.lower()\n",
    "    for kw in LAYOFF_KEYWORDS:\n",
    "        if re.search(kw, text):\n",
    "            return True\n",
    "    return False"
   ],
   "id": "456f51b486ca1b9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dictionary Lookup Method",
   "id": "9f6a684e38aa8b4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# results = {}   # filename -> list of (para, flags)\n",
    "# counter = 0\n",
    "#\n",
    "# for filepath in glob.glob('wrds_clean_filings_1994/*.txt'):\n",
    "#     if counter >= 20: break             #Number of 8-Ks to process\n",
    "#     counter += 1\n",
    "#     with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "#         text = f.read()\n",
    "#     # split on two-or-more newlines\n",
    "#     paras = re.split(r'\\n\\s*\\n', text)\n",
    "#\n",
    "#     matches = []\n",
    "#     for para in paras:\n",
    "#         # skip very short “paras”\n",
    "#         if len(para.strip()) < 50:\n",
    "#             continue\n",
    "#\n",
    "#         guidance = is_guidance_para(para)\n",
    "#         layoff   = is_layoff_para(para)\n",
    "#         if guidance or layoff:\n",
    "#             matches.append({\n",
    "#                 'paragraph': para.strip(),\n",
    "#                 'guidance_flag': guidance,\n",
    "#                 'layoff_flag': layoff\n",
    "#             })\n",
    "#\n",
    "#     if matches:\n",
    "#         results[os.path.basename(filepath)] = matches"
   ],
   "id": "129f22990ff17e98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# for fname, paras in results.items():\n",
    "#     print(f'=== {fname} ===')\n",
    "#     for m in paras:\n",
    "#         flags = []\n",
    "#         if m['guidance_flag']: flags.append('GUIDANCE')\n",
    "#         if m['layoff_flag']:   flags.append('LAYOFF')\n",
    "#         print(f\"[{','.join(flags)}]\\n{m['paragraph']}\\n\")"
   ],
   "id": "6f2483bfc1b7faa9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "FinBERT Pipeline",
   "id": "ee43b156257397e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Process each file ------------------------------------------------\n",
    "\n",
    "def extract_paragraphs(text):\n",
    "    # split on two-or-more newlines\n",
    "    paras = re.split(r\"\\n\\s*\\n\", text)\n",
    "    return [p.strip() for p in paras if len(p.strip()) >= MIN_LEN]\n",
    "\n",
    "\n",
    "results = {}  # filename → list of (paragraph, {label:score})\n",
    "counter = 0\n",
    "for filepath in glob.glob(os.path.join(INPUT_FOLDER, \"*.txt\")):\n",
    "    if counter >= 20:\n",
    "        break\n",
    "    counter += 1\n",
    "    fname = os.path.basename(filepath)\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    paras = extract_paragraphs(text)\n",
    "    if not paras:\n",
    "        continue\n",
    "\n",
    "    # embed all paragraphs in this file\n",
    "    para_embs = model.encode(paras, convert_to_tensor=True)\n",
    "\n",
    "    matches = []\n",
    "    for idx, emb in enumerate(para_embs):\n",
    "        # compute max‐similarity vs each seed category\n",
    "        scores = {\n",
    "            label: util.pytorch_cos_sim(emb, seed_emb).max().item()\n",
    "            for label, seed_emb in seed_embeddings.items()\n",
    "        }\n",
    "        # pick any above threshold\n",
    "        hits = {lbl:s for lbl,s in scores.items() if s >= THRESHOLD}\n",
    "        if hits:\n",
    "            matches.append((paras[idx], hits))\n",
    "\n",
    "    if matches:\n",
    "        results[fname] = matches"
   ],
   "id": "1e7d36e1914f649a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for fname, paras in results.items():\n",
    "    print(f\"\\n=== {fname} ===\")\n",
    "    for para, hits in paras:\n",
    "        lbls = \", \".join(f\"{lbl} ({score:.2f})\" for lbl, score in hits.items())\n",
    "        print(f\"[{lbls}]\\n{para}\\n\")\n"
   ],
   "id": "2706e1271a197055",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Sentiment analysis with FinBERT and Llama",
   "id": "b4654b75825a58b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# FinBERT sentiment classifier\n",
    "finbert_clf = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"ProsusAI/finbert\",\n",
    "    tokenizer=\"ProsusAI/finbert\",\n",
    "    return_all_scores=False\n",
    ")\n",
    "\n",
    "# Llama instruction-based sentiment via text-generation\n",
    "# Attempt to load Llama; if gated, fall back\n",
    "try:\n",
    "    llama_llm = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        tokenizer=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Warning: Could not load Llama-2-7b-chat-hf:\", e)\n",
    "    llama_llm = None\n",
    "\n",
    "print(\"\\n=== Sentiment Comparison ===\")\n",
    "fin = []\n",
    "for fname, paras in results.items():\n",
    "    print(f\"\\n*** {fname} ***\")\n",
    "    for para, _ in paras:\n",
    "        # Run FinBERT\n",
    "        fin = finbert_clf(para[:512])[0]  # truncate to 512 tokens\n",
    "        # Run Llama if available\n",
    "        prompt = (\n",
    "            \"Classify the sentiment of the following paragraph \"\n",
    "            \"as positive, neutral, or negative, and respond with just the label.\\n\\n\"\n",
    "            f\"Paragraph:\\n{para}\\n\\nSentiment:\"\n",
    "        )\n",
    "        if llama_llm is not None:\n",
    "            llama_out = llama_llm(prompt, max_length=20)\n",
    "            llama_label = llama_out[0][\"generated_text\"].strip().split(\"\\n\")[0]\n",
    "        else:\n",
    "            llama_label = \"Llama unavailable\"\n",
    "        # Print results\n",
    "        print(\"Paragraph:\", para)\n",
    "        print(f\"FinBERT -> label: {fin['label']}, score: {fin['score']:.2f}\")\n",
    "        print(f\"Llama   -> label: {llama_label}\")\n",
    "        print(\"-\" * 80)"
   ],
   "id": "80940f1c8f45c3a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Return sentiment score for each file\n",
    "\n",
    "finbert = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"ProsusAI/finbert\",\n",
    "    tokenizer=\"ProsusAI/finbert\",\n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "def sentiment_score(text):\n",
    "    # truncate to 512 tokens to keep it fast\n",
    "    scores = finbert(text[:512])[0]\n",
    "    # weighted average of our mapping\n",
    "    return sum(mapping[d[\"label\"]] * d[\"score\"] for d in scores)\n",
    "\n",
    "for filepath in sorted(glob.glob(f\"{INPUT_FOLDER}/*.txt\"))[:20]:\n",
    "    fname = os.path.basename(filepath)\n",
    "    txt = open(filepath, encoding=\"utf-8\", errors=\"ignore\").read()\n",
    "    # split into paras and drop very short ones\n",
    "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", txt)\n",
    "             if len(p.split()) >= MIN_LEN]\n",
    "\n",
    "    # compute a sentiment score per para\n",
    "    para_scores = [sentiment_score(p) for p in paras]\n",
    "    if not para_scores:\n",
    "        continue\n",
    "\n",
    "    # (a) Print all paragraph scores:\n",
    "    scores_str = \", \".join(f\"{s:.2f}\" for s in para_scores)\n",
    "    print(f\"{fname}: {scores_str}\")\n",
    "\n",
    "    # Print the file’s average score:\n",
    "    avg = sum(para_scores)/len(para_scores)\n",
    "    print(f\"{fname}  ⟶  average sentiment intensity: {avg:.2f}\\n\")"
   ],
   "id": "6a109fe953033efb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
